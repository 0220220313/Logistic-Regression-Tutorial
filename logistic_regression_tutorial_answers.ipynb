{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "In regression we were trying to predict a continuous value - e.g. exam score.\n",
    "\n",
    "In classification we predict a class label for a given data point - e.g. pass/fail the exam. It might be a two-class problem like this (binary classification) or a many-class problem.\n",
    "\n",
    "Not only do we predict a class label, we also predict a probability for each class.\n",
    "\n",
    "Our example of pass fail can be pictured like this\n",
    "\n",
    "![logistic_regression](https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg)\n",
    "\n",
    "To perform classification we need to make a **decision**. This requires defining a **decision boundary**. This is going to be an affine set - in 1D (i.e. with one feature) this will be a point; in 2D (with two features) this will be a line; with three features a plane; etc...\n",
    "\n",
    "In our case above we could say if `num_hours_studying` $\\gt 2.7$ then we will predict `pass`.\n",
    "\n",
    "In a bivariate case our decision boundary might look like this\n",
    "\n",
    "![bivariate_logistic_regression](https://i0.wp.com/ucanalytics.com/blogs/wp-content/uploads/2017/09/Scatter-Plot-with-Boundary-Logistic-Regression.jpg?resize=768%2C578)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "Our hypothesis for logistic regression can be linear or non-linear. What is different with linear regression is we want to output class probabilities. We need a way to express our hypothesis as a probability value.\n",
    "\n",
    "To do this we use the sigmoid function\n",
    "\n",
    "$$\\sigma(h) = \\frac{e^h}{e^h + 1} = \\frac{1}{1 + e^{-h}}$$\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "We can see that this function is going to squash its input into the range $(0, 1)$ giving us a valid probability value.\n",
    "\n",
    "We will pass our linear or non-linear function of the data through the sigmoid function to get our final hypothesis\n",
    "\n",
    "$$h_\\theta(\\mathbf{x}) = P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$\n",
    "\n",
    "We can arbitrarily decide what $y=1$ means - in the example let's say passes the exam. Then the probability of failing the exam is given by\n",
    "\n",
    "$$P(y=0|\\mathbf{x}) = 1 - P(y=1|\\mathbf{x}) = 1 - \\sigma(\\mathbf{a}^\\top\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "MSE is not an appropriate loss function here. We are dealing with probabilities so the natural choice is negative log loss\n",
    "\n",
    "$$\\text{NLL} = -y \\log(h_\\theta) - (1 - y)\\log(1 - h_\\theta))$$\n",
    "\n",
    "$y$ is going to be either $1$ or $0$ so only one of these two terms will apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We cannot use a closed form here so we will have to use **gradient descent**.\n",
    "\n",
    "When we learned about linear regression we noted that the loss surface was convex - from any point you know how to reach the global minimum.\n",
    "\n",
    "Our loss here is highly non-convex. Instead we need to use an iterative algorithm to take clever steps from an initial starting point to try and find a good local minimum. The picture looks like this\n",
    "\n",
    "![gradient_descent](https://cdn-images-1.medium.com/max/1600/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "The key idea is that if you take the gradient of the parameters with respect to the loss at a particular point, it will tell you how to change those parameters to reduce the loss\n",
    "\n",
    "![gradient_descent_2](https://cdn-images-1.medium.com/max/800/0*rBQI7uBhBKE8KT-X.png)\n",
    "\n",
    "Clearly in this diagram if we start on the right, we need to **decrease** $w$ in order to lower our loss. That's really the main idea.\n",
    "\n",
    "With this algorithm we make a number of \"steps\" to update an improve the parameters. Each step the update is\n",
    "\n",
    "$$\\theta' = \\theta - \\beta \\frac{\\partial \\text{NLL}(h_\\theta, y)}{\\partial \\theta}$$\n",
    "\n",
    "Here the hyperparameter $\\beta$ is very important - it controls how big our steps are. In the above diagram if the learning rate made us step half the width of the \"U\" we would just bounce from side to side and never settle in the optimum.\n",
    "\n",
    "We also need to pay attention to how many steps we take. In the diagram if we just took two we wouldn't give the algorithm time to settle. In practice we will take many more steps than two. Be aware that you will have to tune this hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "A retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa. There are roughly two controls per case of coronary heart disease (CHD). Many of the CHD positive men have undergone blood pressure reduction treatment and other programs to reduce their risk factors after their CHD event. In some cases the measurements were made after these treatments. These data are taken from a larger dataset, described in  Rousseauw et al, 1983, South African Medical Journal.  Downloaded from https://web.stanford.edu/~hastie/ElemStatLearn/.\n",
    "\n",
    "Features:\n",
    "- sbp: systolic blood pressure\n",
    "- tobacco:\tcumulative tobacco (kg)\n",
    "- ldl: low densiity lipoprotein cholesterol\n",
    "- adiposity\n",
    "- famhist: family history of heart disease (Present, Absent)\n",
    "- typea: type-A behavior\n",
    "- obesity\n",
    "- alcohol: current alcohol consumption\n",
    "- age: age at onset\n",
    "- chd: response, coronary heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "### Load the Data\n",
    "\n",
    "As usual, load the data with `pd.read_csv`. We have an index column in position zero and the separator is a comma. Load the data into a variable named `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: load the data into a variable named data\n",
    "\"\"\"\n",
    "file_name = 'SAheart.data'\n",
    "data = pd.read_csv(file_name, sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert len(data) == 462\n",
    "assert len(data.columns) == 10\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row.names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5.73</td>\n",
       "      <td>23.11</td>\n",
       "      <td>Present</td>\n",
       "      <td>49</td>\n",
       "      <td>25.30</td>\n",
       "      <td>97.20</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.41</td>\n",
       "      <td>28.61</td>\n",
       "      <td>Absent</td>\n",
       "      <td>55</td>\n",
       "      <td>28.87</td>\n",
       "      <td>2.06</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.48</td>\n",
       "      <td>32.28</td>\n",
       "      <td>Present</td>\n",
       "      <td>52</td>\n",
       "      <td>29.14</td>\n",
       "      <td>3.81</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>7.50</td>\n",
       "      <td>6.41</td>\n",
       "      <td>38.03</td>\n",
       "      <td>Present</td>\n",
       "      <td>51</td>\n",
       "      <td>31.99</td>\n",
       "      <td>24.26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.50</td>\n",
       "      <td>27.78</td>\n",
       "      <td>Present</td>\n",
       "      <td>60</td>\n",
       "      <td>25.99</td>\n",
       "      <td>57.34</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  \\\n",
       "row.names                                                                    \n",
       "1          160    12.00  5.73      23.11  Present     49    25.30    97.20   \n",
       "2          144     0.01  4.41      28.61   Absent     55    28.87     2.06   \n",
       "3          118     0.08  3.48      32.28  Present     52    29.14     3.81   \n",
       "4          170     7.50  6.41      38.03  Present     51    31.99    24.26   \n",
       "5          134    13.60  3.50      27.78  Present     60    25.99    57.34   \n",
       "\n",
       "           age  chd  \n",
       "row.names            \n",
       "1           52    1  \n",
       "2           63    1  \n",
       "3           46    0  \n",
       "4           58    1  \n",
       "5           49    1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage Categorial Variable\n",
    "\n",
    "We once again have a categorical variable, `famhist`, which we need to make into a one-hot encoding. We do this for you this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['famhist_true'] = data['famhist'] == 'Present'\n",
    "data['famhist_false'] = data['famhist'] == 'Absent'\n",
    "data = data.drop(['famhist'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will perform feature selection later, this time let's take the time to make a convenient function to separate a dataset into `train` and `test` and also split $x$ and $y$ in boths sets for us. We do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(data):\n",
    "    # control randomization for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    train, test = model_selection.train_test_split(data)\n",
    "    x_train = train.loc[:, train.columns != 'chd']\n",
    "    y_train = train['chd']\n",
    "    x_test = test.loc[:, test.columns != 'chd']\n",
    "    y_test = test['chd']\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's now take a look at the relationships in the data. We can still use scatterplots for classification, but they look a bit different. Again let's make a convenient function for later use. As you will see we stretch the plot size to try and make these plots a bit clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(_data, feature_name):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.scatter(_data[feature_name], _data['chd'])\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('response')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at one feature together then you should explore the rest of the dataset on your own and form your own opinions about which features are going to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAADTCAYAAAAvQQ9YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH8RJREFUeJzt3X20XGV96PHvbyYTPCeCJ4GjlZPEICJcEAhySrD0VuCqICqigiGFXt+uaau2drXmXmhdmlC9vrBEXau2lnqt2iqK1stNKy3tQrzt4hYkGATDiyKiEChEIIgkkJPkd/+YmcOcOTPnzHmZ7HMy389aWZm959nP/j3Pfmbv39kvM5GZSJIkqTilogOQJEnqdSZkkiRJBTMhkyRJKpgJmSRJUsFMyCRJkgpmQiZJklQwEzJJkqSCmZBJkiQVzIRMkiSpYAuKDmCqDjnkkFyxYkXRYUiSJE3q5ptv/nlmDk5Wbt4lZCtWrGDTpk1FhyFJkjSpiPhpJ+W8ZClJklQwEzJJkqSCmZBJkiQVzIRMkiSpYCZkkiRJBevaU5YR8XngtcDDmfmSFu8H8GngLGAH8NbM/F634pnPrtq8lUuvuYsHtu/k0IE+1p1xJOecMDQn1t34/kB/hUx4fOfIhHF20p7ptLndMjPpv+ZlTztqkOvu3NZxXe+/6jauuPE+9mSOzgugPrW4v8IHX3fMaJzrN25h+86Rce9Nt90b/n4Lj+0YGS070Ffhtcc/n+vu3MbW7TspR7Ank6GGtjXPX3fGkQDj1rHpp4+Otq0cwZpVy/jQOcdOGM/F37yVnSN7ASgF/Oaq5XzonGNb9lPdooVl3vDSoXGxTRZ7f6XEzt17aaxyqM24aB67zdu5sf66Um1D7u0w3kULyzy5a8+E23KoxZhq1TelgL3ju6rlNo6AFt0KjB2Lja9b1XnMoQdywz2Pjfb9yS9czL2P7Gw5XurxN4/puvp4GX7BkgnH/ETjYrQNtfbVx8G3bn1wdMz3V0osXFDm8Z0jPKevwq7de9gx0rzFnompcTw11lNX7/d62cW1cdPYvua+adWv/ZUSB1TKbN9RHW8rDu4b7dtG9f6A8Z8/YMznu76e5tia98f1cd84lutajb9ONX6e+pvGeuNnYrb3+1ON7dAJ9mn76rjaTuQEA31GFUf8BvBL4EttErKzgN+jmpCtAj6dmasmq3d4eDh76Wsvqgex29g58szg7quU+cgbj+364Jls3a3eb9Qqzk7aM502t1vmTScO8Xc3b51W/03Wvsnqev9Vt/G3N/xswnUAVMrB6l9dxte+ex8jTUfZSjm49Nzjp9Xur910HyN7Zv75rpQDkjGxlUvBnhYZwSmHL+F7P3u8ZTxfueFn45IXgCOeu4gfPfzkjOPsVLtxMVc0jqlOx9BcUo8fYN3Xvz9uTE+mPuY3/fTRedf2biiXghJjP3+VUrAXWn4G2+l03E/n+NLJvnKy9XTrWNeq3lb7tG4eVyPi5swcnrRctxKyWhArgH9ok5D9JfCdzLyiNn0XcGpmPjhRnb2WkJ3y0W+3/Uvm+otOL3Td7d5vVbbTOjst02ms9b8Wp1LXZHV2WtfhF1894V/2ncQ5WaxTbXdRjGdq6tt8KmNoLhka6APo6PPTbvn/ePypedn2uazTcT/V40un+8qJ1tOtY91UYuvWcbXThKzIL4YdAu5rmL6/Nm9cQhYRa4G1AMuXL98nwc0VD7QZSO3m78t1dxJDc5lO2jOdNrd7r93OZzqxz9a6p1p2NttdFOOZmvp2netxtjPT/dMD23e2vXyq6et0PE11+013e890vz/Vdcxm2W6YFzf1Z+blmTmcmcODg5P++sB+5dDaX5qdzt+X6+4khuYynbRnOm1u9145Ysp1TaXMdNY91bKz2e6iGM/U1LfrXI+znUMH+ma0jzp0oG/etn0u67RPp7rtprutZ7rfn+o6ZrNsNxSZkG0FljVML63NU4N1ZxxJX6U8Zl5fpTx6U2KR6271fruyndbZaZlOY12zatm0+2+y9k1W15pVy1rOb1YpV29wrpTG7ywr5Zh2uyvl2TmgVcoxLrZyi1iheg9Zu3ja7WyOeO6i2QizY+3GxVzROKY6HUNzST3+dWcc2XJMT6Y+5udj27uhXBr/+auUou1nsJ1Ox/10ji+d7CsnW0+3jnWt6m21T9tXx9WJlNevX9+1yjds2DAA/Ob69ev/vMV7Cbxzw4YNX9mwYcPJwOmZ+cnJ6rz88svXr127tgvRzk1HPf8gli7u47atj/PLp3YzNNDHB1539D55GmSydTe/v7i/wrMWlHl69962cXbSnum0ud0y7zrtRdPuv1Z1vn7loTzyy10d1XX6Uc/j5798mi1bfzHm8kvjbmBxf4UPv+FY3nXai1i+pJ8b7nmEp3bvHfPedNu9fEk/N/7kEZ5qeKpsoK/CucNLeeSXu3jiqd2UI0gY07bm+evPPoZXHfMrY9ax4exjOOTAhaNtK0dwwcnL+cwFJ7aN5wUHL+L//vBhdtdupC0FXHDycr7wtlUt+6lu0cIyq3912bjYJou9v1Jid9MlmnbjonnsNm/nxvrrStH6ycR28S5aWJ70IYvmMdVuDJVi4iciG7fxRCdFos3rVnWe+IIBHtj+1Gjf/9rhS9ibjBsv9fiPev5B48Z0XX28vP2Uw9qO+XZtH9eGWuD17XTfoztGx3x/pcSiAxbw9O69DPRVKAVtHzJoHk+N9dTV+71etj5uGtvX3Det+rW/UuLZz1rA0yPV8Xbc0EGjfdtocX+F//mGY8d9/taffQxnHPMrYz7f9fU0x9a4P24c941juW66x5fm/VDzWG/8TMzmfn86sbXbp3XzuLphw4YH169ff/lk5br5lOUVwKnAIcBDwAeBCkBmfrb2tRd/BpxJ9Wsv3paZk96t32s39UuSpPmr8Jv6M3PNJO8n8O5urV+SJGm+mBc39UuSJO3PTMgkSZIKZkImSZJUMBMySZKkgpmQSZIkFcyETJIkqWAmZJIkSQUzIZMkSSqYCZkkSVLBTMgkSZIKZkImSZJUMBMySZKkgpmQSZIkFcyETJIkqWAmZJIkSQUzIZMkSSqYCZkkSVLBTMgkSZIKZkImSZJUsK4mZBFxZkTcFRF3R8RFLd5fHhHXRcTmiLg1Is7qZjySJElzUdcSsogoA58BXg0cDayJiKObir0fuDIzTwDOB/68W/FIkiTNVd08Q3YScHdm3pOZu4CvAq9vKpPAQbXXzwEe6GI8kiRJc1I3E7Ih4L6G6ftr8xqtBy6MiPuBq4Hfa1VRRKyNiE0RsWnbtm3diFWSJKkwRd/Uvwb4QmYuBc4C/iYixsWUmZdn5nBmDg8ODu7zICVJkrqpmwnZVmBZw/TS2rxG7wCuBMjMfweeBRzSxZgkSZLmnG4mZDcBR0TEYRGxkOpN+xubyvwM+C8AEfGfqCZkXpOUJEk9pWsJWWbuBt4DXAPcQfVpyi0RcUlEnF0r9kfAOyPi+8AVwFszM7sVkyRJ0ly0oJuVZ+bVVG/Wb5z3gYbXtwOndDMGSZKkua7om/olSZJ6ngmZJElSwUzIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqmAmZJElSwUzIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqmAmZJElSwTpKyCLixRFxbUT8oDZ9XES8v7uhSZIk9YZOz5D9FXAxMAKQmbcC53crKEmSpF7SaULWn5nfbZq3e7KFIuLMiLgrIu6OiIvalHlzRNweEVsi4isdxiNJkrTfWNBhuZ9HxOFAAkTEucCDEy0QEWXgM8ArgfuBmyJiY2be3lDmCKpn3k7JzMci4rnTaIMkSdK81mlC9m7gcuCoiNgK/AS4cJJlTgLuzsx7ACLiq8DrgdsbyrwT+ExmPgaQmQ9PIXZJkqT9QkcJWS2pekVELAJKmflEB4sNAfc1TN8PrGoq82KAiLgeKAPrM/OfmiuKiLXAWoDly5d3ErIkSdK80elTlu+NiIOAHcAnI+J7EfGqWVj/AuAI4FRgDfBXETHQXCgzL8/M4cwcHhwcnIXVSpIkzR2d3tT/9sz8BfAq4GDgt4CPTrLMVmBZw/TS2rxG9wMbM3MkM38C/JBqgiZJktQzOk3Iovb/WcCXMnNLw7x2bgKOiIjDImIh1a/J2NhU5iqqZ8eIiEOoXsK8p8OYJEmS9gudJmQ3R8Q/U03IromIA4G9Ey2QmbuB9wDXAHcAV2bmloi4JCLOrhW7BngkIm4HrgPWZeYj02mIJEnSfBWZOXmhiBKwErgnM7dHxMHAUO0LYvep4eHh3LRp075erSRJ0pRFxM2ZOTxZuU6fstwbEQ8BR0dEp1+VIUmSpA50lFxFxMeA1VS/Q2xPbXYC/9qluCRJknpGp2e7zgGOzMynuxmMJElSL+r0pv57gEo3A5EkSepVnZ4h2wHcEhHXAqNnyTLz97sSlSRJUg/pNCHbyPjvEJMkSdIs6PQpyy/Wvtz1xbVZd2XmSPfCkiRJ6h2dPmV5KvBF4F6q39C/LCLekpk+ZSlJkjRDnV6y/ATwqsy8CyAiXgxcAZzYrcAkSZJ6RadPWVbqyRhAZv4Qn7qUJEmaFZ2eIdsUEZ8D/rY2fQHg7xdJkiTNgk4Tst8F3g3Uv+bi34A/70pEkiRJPabTpyyfjog/A64F9lJ9ynJXVyOTJEnqEZ0+Zfka4LPAj6k+ZXlYRPx2Zv5jN4OTJEnqBVN5yvK0zLwbICIOB74FmJBJkiTNUKdPWT5RT8Zq7gGe6EI8kiRJPWcqT1leDVwJJHAecFNEvBEgM7/ZpfgkSZL2e50mZM8CHgJeXpveBvQBr6OaoJmQSZIkTVOnT1m+rduBSJIk9aqO7iGLiI9HxEERUYmIayNiW0Rc2O3gJEmSekGnN/W/KjN/AbyW6g+MvwhYN9lCEXFmRNwVEXdHxEUTlHtTRGREDHcYjyRJ0n6j04SsfmnzNcDXM/PxyRaIiDLwGeDVwNHAmog4ukW5A4H3Ajd2GIskSdJ+pdOE7B8i4k7gRODaiBgEnppkmZOAuzPzntq3+n8VeH2Lcn8KfKyD+iRJkvZLHSVkmXkR8GvAcGaOADtonVw1GgLua5i+vzZvVES8FFiWmd+aqKKIWBsRmyJi07Zt2zoJWZIkad7o9Kb+fuBdwF/UZh0KzOh+r4goAZcBfzRZ2cy8PDOHM3N4cHBwJquVJEmaczq9ZPnXwC6qZ8kAtgIfmmSZrcCyhumltXl1BwIvAb4TEfcCJwMbvbFfkiT1mk4TssMz8+PACEBm7qD6I+MTuQk4IiIOi4iFwPnAxvqbmfl4Zh6SmSsycwVwA3B2Zm6aaiMkSZLms04Tsl0R0Uf1W/nrPy7+9EQLZOZu4D3ANcAdwJWZuSUiLomIs2cQsyRJ0n5l0m/qj4gAPgv8E7AsIr4MnAK8dbJlM/Nq4OqmeR9oU/bUycOVJEna/0yakGVmRsQ64FSq93kF8N7M/HmXY5MkSeoJnf64+PeAF0729RSSJEmauk4TslXABRHxU+BJqmfJMjOP61pkkiRJPaLThOyMrkYhSZLUwzpKyDLzp90ORJIkqVd1+rUXkiRJ6hITMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqmAmZJElSwUzIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqWFcTsog4MyLuioi7I+KiFu//YUTcHhG3RsS1EfGCbsYjSZI0F3UtIYuIMvAZ4NXA0cCaiDi6qdhmYDgzjwO+AXy8W/FIkiTNVd08Q3YScHdm3pOZu4CvAq9vLJCZ12XmjtrkDcDSLsYjSZI0J3UzIRsC7muYvr82r513AP/Y6o2IWBsRmyJi07Zt22YxREmSpOLNiZv6I+JCYBi4tNX7mXl5Zg5n5vDg4OC+DU6SJKnLFnSx7q3AsobppbV5Y0TEK4A/AV6emU93MR5JkqQ5qZtnyG4CjoiIwyJiIXA+sLGxQEScAPwlcHZmPtzFWCRJkuasriVkmbkbeA9wDXAHcGVmbomISyLi7FqxS4FnA1+PiFsiYmOb6iRJkvZb3bxkSWZeDVzdNO8DDa9f0c31S5IkzQdz4qZ+SZKkXmZCJkmSVDATMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqmAmZJElSwUzIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBXMhEySJKlgJmSSJEkFMyGTJEkqmAmZJElSwUzIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBVsQTcrj4gzgU8DZeBzmfnRpvcPAL4EnAg8AqzOzHu7GdNkrtq8lUuvuYsHtu/k0IE+1p1xJMC4eeecMNSVdTXXe9XmrWz4+y08tmMEgIG+CoMHLuRHDz85WqZSgtUnLee6O7eN1nXaUYP83c33s3Nk72i5vkqJN524lOvu3MbW7TspR7Anc9z/Q02xNMbZVymxc/deMp+Jsb5cOwN9FV57/PNH1zuZABprKwUcsKDEUyN7x/TTVZu3sn7jFrbvrPbN4v4Krznu+aP98Jy+CiN79vLkrj0t17NoYZkPv+HYce3sJMZ2jnjuIv7lD0/lqs1bufibt47p/zFtDLhg1XKGX7BkdJ31fhzoqxAB23eMjGtDvW8m6vN6u4AJ27NoYZlKucT2nSOj9UXABJsSgP5KiTeeuJSvffdntGleS6WAl71wCVseeGJ0m5UC9iYMDfSx4uA+/t+PH2WS1Y9TKcGCcmlMXzePoXq5S89bCcAff/NWdkwleGBBKXj2AQvYvnNkTD+VgKnVNF5/pcQBlTLbd4ww0F8hEx7fOfb1VPuluf7Gz22r/mnloAPKPLlr74Sf74lUSrQdI/Vtfv2PHx0zf9HCMk/u2jMuxvp0AAsXlHh6d+uK62NqdEw31dNfKbFwQXlM/zZ+BvorpSmPDXhmP/etWx8cs79ef/YxwDOfxcaxs7i/wgdfN/b9dp/tvkq1zfW2rVm1jA+dc+yY/VbjPvy0owbHHBPq+833X3UbV9x43+g+v17PTHT7uNnJsbIby+5rkdP8oE1acUQZ+CHwSuB+4CZgTWbe3lDmXcBxmfk7EXE+8IbMXD1RvcPDw7lp06auxFw9iN7GzpFnDuCVckDCyN5n+qmvUuYjbzx2Rhu11bqa671q81bWfeP7jOzpzjaaSD0WYFycReurlHnTiUN87bv3jdku01EuBZ8473hg9tr5vAMXsu2JXR0dpOsHj9lWiupOe6b9I2lmSkC5HG3346Wo7oems58/5fAlfO9nj3e03+qrlHnp8ueMS4ABLjx5+bSTspbHzVJAMKZN0z1udnKs7Maysykibs7M4UnLdTEhexmwPjPPqE1fDJCZH2koc02tzL9HxALgP4DBnCCobiZkp3z02x2fHRka6OP6i06f9XU11juVeLphaKAPoNAY2pnsrNxUzOV2SlK3lSP48UfOmtay3T5udnKs7Mays6nThKyblyyHgPsapu8HVrUrk5m7I+Jx4GDg542FImItsBZg+fLl3YqXB6ZwQJ5K2aks3zh/puuYqaLXP5HZSsZgbrdTkrptJvvTbh83OzlWdmPZIsyLm/oz8/LMHM7M4cHBwa6t59DamZLZLjuV5Rvnz3QdM3XoQF/hMbRTjpi1uuZyOyWp22ayP+32cbOTY2U3li1CNxOyrcCyhumltXkty9QuWT6H6s39hVh3xpH0Vcpj5lXKUb0e3qCvUh69aXE219Vc77ozjqzew1aAeiyt4ixaX6XMmlXLxm2X6SiXYtbb+bwDF3b8wZqFJrStdzb6R9LMlGDC/XgpJn5/IqccvqTj/VZfpcwphy9p+d6aVctazu9Ey+NmKca1abrHzU6Old1YtgjdTMhuAo6IiMMiYiFwPrCxqcxG4C211+cC357o/rFuO+eEIT7yxmMZGugjqF5nvvTc47n0vOPHzJuNGwJbrau53nNOGOLSc49ncX9ldN5AX4UjnrtoTF2VUvWmzMa6Ljx5OX2VsZu3r1IaLQfP/FXU/H9jLM1x9ldKNP8xNdlfVwN9lTHrnUxzbaWoxt7YTx8651guPe94Bvqe6ZvF/ZUx/TDQV2HRwvY7q0ULy3zivOPHtXMmjnjuIm78k1dy2eqV4/p/TBujus0ue/PKcdtjoK/C4v5KyzbU+2aiPl+0sMxlb145Om4nKlfvv3p9nfyh3F8bRxM0r6VSVA8gjdusnjMODfRxyuFLxm37TlRKjOvrVvVUSvCp1Sv51OqV9E81eKpPWdZjb+yn2diJ9ldKo9t8cX+l+qRt0+uZ1t8Yc6f1HXRAeUZnTybq5vo2b1Yf781rjYb/D1jQvuL6mBod003v91dK4/q3sfx0xgY8s59r3l9ftnoll577zGexsTsX91eqn9WG99v1d1+lNKZtF568nC+/82Vj9luN+/DmY8JH3ngsX37ny7jw5OVj9vkzuaEf2hw3zzt+tE0zPW52cqzsxrJF6NpN/QARcRbwKapfe/H5zPxwRFwCbMrMjRHxLOBvgBOAR4HzM/Oeiers5k39kiRJs2ku3NRPZl4NXN007wMNr58CzutmDJIkSXPdvLipX5IkaX9mQiZJklQwEzJJkqSCmZBJkiQVrKtPWXZDRGwDnqTp2/w16hDsm3bsm/bsm/bsm9bsl/bsm/Z6sW9ekJmTfqv9vEvIACJiUyePkPYi+6Y9+6Y9+6Y9+6Y1+6U9+6Y9+6Y9L1lKkiQVzIRMkiSpYPM1Ibu86ADmMPumPfumPfumPfumNfulPfumPfumjXl5D5kkSdL+ZL6eIZMkSdpvmJBJkiQVbF4lZBFxb0TcFhG3RMSmouMpWkR8PiIejogfNMxbEhH/EhE/qv2/uMgYi9CmX9ZHxNba2LklIs4qMsaiRMSyiLguIm6PiC0R8d7afMdN+77p+bETEc+KiO9GxPdrfbOhNv+wiLgxIu6OiK9FxMKiY93XJuibL0TETxrGzcqiYy1KRJQjYnNE/ENtuufHTSvzKiGrOS0zV/o9JgB8ATizad5FwLWZeQRwbW2613yB8f0C8Mna2FmZmVfv45jmit3AH2Xm0cDJwLsj4mgcN9C+b8Cx8zRwemYeD6wEzoyIk4GPUe2bFwGPAe8oMMaitOsbgHUN4+aW4kIs3HuBOxqmHTctzMeETDWZ+a/Ao02zXw98sfb6i8A5+zSoOaBNvwjIzAcz83u1109Q3UkO4biZqG96Xlb9sjZZqf1L4HTgG7X5vTpu2vWNgIhYCrwG+FxtOnDctDTfErIE/jkibo6ItUUHM0c9LzMfrL3+D+B5RQYzx7wnIm6tXdLsuUtyzSJiBXACcCOOmzGa+gYcO/XLTrcADwP/AvwY2J6Zu2tF7qdHE9jmvsnM+rj5cG3cfDIiDigwxCJ9CvjvwN7a9ME4blqabwnZr2fmS4FXU72c8BtFBzSXZfU7TfxLreovgMOpXlJ4EPhEseEUKyKeDfwd8AeZ+YvG93p93LToG8cOkJl7MnMlsBQ4CTiq4JDmjOa+iYiXABdT7aNfBZYA/6PAEAsREa8FHs7Mm4uOZT6YVwlZZm6t/f8w8L+p7hQ01kMR8XyA2v8PFxzPnJCZD9V2mnuBv6KHx05EVKgmHF/OzG/WZjtuaN03jp2xMnM7cB3wMmAgIhbU3loKbC0ssDmgoW/OrF0Cz8x8GvhrenPcnAKcHRH3Al+leqny0zhuWpo3CVlELIqIA+uvgVcBP5h4qZ60EXhL7fVbgP9TYCxzRj3ZqHkDPTp2avdv/C/gjsy8rOGtnh837frGsQMRMRgRA7XXfcArqd5jdx1wbq1Yr46bVn1zZ8MfOEH1HqmeGzeZeXFmLs3MFcD5wLcz8wIcNy3Nm2/qj4gXUj0rBrAA+EpmfrjAkAoXEVcApwKHAA8BHwSuAq4ElgM/Bd6cmT11g3ubfjmV6iWnBO4FfrvhnqmeERG/DvwbcBvP3NPxx1Tvler1cdOub9bQ42MnIo6jevN1meof8ldm5iW1/fJXqV6S2wxcWDsj1DMm6JtvA4NAALcAv9Nw83/PiYhTgfdl5msdN63Nm4RMkiRpfzVvLllKkiTtr0zIJEmSCmZCJkmSVDATMkmSpIKZkEmSJBXMhEzSfici3hoRf1Z7/TsR8V9nse5DI+IbtdcrI+Ks2apbUu9aMHkRSZq/MvOzs1zfAzzzpZYrgWHg6tlch6Te4xkySfNORFwVETdHxJaIWFub97aI+GFEfJfqT7bUy66PiPfVXn8nIj4dEbdExA8i4qTa/CW1Om+NiBtqX/ZJRLy8VvaWiNgcEQdGxIrasguBS4DVtfdXR8SPImKwtmwpIu6uT0vSRDxDJmk+entmPlr7qZqbIuJbwAbgROBxqj/NsrnNsv2ZuTIifgP4PPCS2rKbM/OciDgd+BLVs1/vA96dmdfXfnT8qXolmbkrIj4ADGfmewAi4ijgAuBTwCuA72fmtllvvaT9jmfIJM1Hvx8R3wduAJYBvwV8JzO3ZeYu4GsTLHsFQGb+K3BQ7XcIfx34m9r8bwMHR8RBwPXAZRHx+8BAZu6eJK7PA/X71d5O9UelJWlSJmSS5pXab+K9AnhZZh5P9UzYnVOoovn34tr+flxmfhT4b0AfcH3tDFj7ijPvAx6qnWU7CfjHKcQlqYeZkEmab54DPJaZO2oJ0slUE6aXR8TBEVEBzptg+dUw+mPij2fm41R/VPyC2vxTgZ9n5i8i4vDMvC0zPwbcBDQnZE8ABzbN+xzwt8DXM3PPTBoqqXeYkEmab/4JWBARdwAfpXrZ8kFgPfDvVC8z3jHB8k9FxGbgs8A7avPWAydGxK21Ot9Sm/8HtRv4bwVGGH/G6zrg6PpN/bV5G4Fn4+VKSVMQmW3P1kvSfiUivgO8LzM3dXEdw8AnM/M/d2sdkvY/PmUpSbMkIi4Cfpfa5U9J6pRnyCRJkgrmPWSSJEkFMyGTJEkqmAmZJElSwUzIJEmSCmZCJkmSVLD/Dw0We1kJ3nPPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature(data, 'adiposity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a reasonable correlation here - this might be a useful feature. Explore the rest yourself with our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: Explore the relationships in the data using the helper function `plot_feature`\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Baseline Model\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Since we are predicting the label of our classes (heart disease or none), we have a much more intuitive measure of performance: prediction accuracy. To calculate this we will use `sklearn.metrics.accuracy_score` - reference here http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_train, y_train, x_test, y_test):\n",
    "    train_preds = model.predict(x_train)\n",
    "    test_preds = model.predict(x_test)\n",
    "    train_acc = metrics.accuracy_score(y_train, train_preds)\n",
    "    test_acc = metrics.accuracy_score(y_test, test_preds)\n",
    "    print('Train accuracy: %s' % train_acc)\n",
    "    print('Test accuracy: %s' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Model\n",
    "\n",
    "Here I will implement a baseline gradient descent model. I will not perform feature selection or tune regularization. You will then need to beat this baseline.\n",
    "\n",
    "The baseline use the `sklearn.linear_model.SGDClassifier` class - reference here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier. By passing the argument `loss='log'` we get a logistic regression model.\n",
    "\n",
    "I append `bl` to the variables here to mark them as the baseline. Let's also add one more convenient helper function that will split the data, train the model, and return train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6994219653179191\n",
      "Test accuracy: 0.6982758620689655\n"
     ]
    }
   ],
   "source": [
    "def split_train_evaluate(model, data):\n",
    "    x_train, y_train, x_test, y_test = splits(data)\n",
    "    model.fit(x_train, y_train)\n",
    "    evaluate(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "model_bl = linear_model.SGDClassifier(loss='log', max_iter=10000)\n",
    "split_train_evaluate(model_bl, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat the Baseline\n",
    "\n",
    "What may be difficult about this task is that for the first time during our tutorials we have a number of hyperparameters. This is the common situation in machine learning practice. One approach to dealin with this issue is to define a reasonable set of values for each hyperparameter and then search over all combinations of them using cross validation on the training set. This technique is called **grid search**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "We can perform gid search with `sklearn.model_selection.GridSearchCV`, reference here http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html.\n",
    "\n",
    "Let's try it out with a very small search space to show you how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.01, 0.1, 1.0], 'max_iter': [1000, 10000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = splits(data)\n",
    "grid_search = model_selection.GridSearchCV(\n",
    "    estimator=linear_model.SGDClassifier(),\n",
    "    param_grid={'alpha': [0.01, 0.1, 1.],\n",
    "                'max_iter': [1000, 10000]},\n",
    "    cv=10,\n",
    "    return_train_score=True)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search results come in a dictionary that can be loaded directly into a pandas `DataFrame` for viewing so you can see what the parameter choices did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'alpha': 0.01, 'max_iter': 1000}</td>\n",
       "      <td>0.633707</td>\n",
       "      <td>0.630058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'alpha': 0.01, 'max_iter': 10000}</td>\n",
       "      <td>0.728646</td>\n",
       "      <td>0.699422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'alpha': 0.1, 'max_iter': 1000}</td>\n",
       "      <td>0.708455</td>\n",
       "      <td>0.696532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'alpha': 0.1, 'max_iter': 10000}</td>\n",
       "      <td>0.726074</td>\n",
       "      <td>0.722543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'alpha': 1.0, 'max_iter': 1000}</td>\n",
       "      <td>0.712268</td>\n",
       "      <td>0.690751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               params  mean_train_score  mean_test_score\n",
       "0   {'alpha': 0.01, 'max_iter': 1000}          0.633707         0.630058\n",
       "1  {'alpha': 0.01, 'max_iter': 10000}          0.728646         0.699422\n",
       "2    {'alpha': 0.1, 'max_iter': 1000}          0.708455         0.696532\n",
       "3   {'alpha': 0.1, 'max_iter': 10000}          0.726074         0.722543\n",
       "4    {'alpha': 1.0, 'max_iter': 1000}          0.712268         0.690751"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.DataFrame(grid_search.cv_results_)\n",
    "# we only want a subset of the columns for a precise summary\n",
    "r[['params', 'mean_train_score', 'mean_test_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid seach also picks the best model for you automatically which you can then use with the optimal parameters from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7485549132947977\n",
      "Test accuracy: 0.7241379310344828\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "evaluate(best_model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have already improved our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "You can use the grid search code above to find your best model and beat our baseline test accuracy of $72.4$ percent.\n",
    "\n",
    "You will want to examine the parameters in the `SGDClassifier` documentation (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) and make your own decisions about what a reasonable search space should look like.\n",
    "\n",
    "Also be aware that if your best value for a parameter is on the edge of your search space, you want to expand the space further in that direction to see if you can keep climbing. For example if my space for $\\alpha$ was `[0.1, 0.5, 1.]`, and the best result came with `1.`, then I should definitely try `2.` and `5.` and so on.\n",
    "\n",
    "Also don't forget feature selection. Return to the section where we were visualizing. Try and apply some $L2$ penalty through the parameters to perform ridge regression if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: Beat the baseline 72.4% test accuracy\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
